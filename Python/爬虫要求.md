

# 爬虫要求

* 爬首页的所有文章（分析首页的文章信息）－**4000条数据**
* 爬热门博主（前3000名）的用户信息和活动信息和博客的文章（分析热门博主的用户信息和活动信息和文章内容）－**用户信息3000条，活动信息23万条，文章内容2万条**
*  爬所有的**待解决**的问题列表（分析谁最爱问问题，最爱问什么样的问题，问题的类型是什么）－**4万条数据**

# 技术要求

* mongodb的使用（完成）
* redis的使用（完成）
* 动态登录网站
* 获取js运行的动态内容
* 文件的下载（完成）
* 动态的控制访问速度（完成）
* 增加log信息（完成）
* 爬虫状态的查看
* 模拟登录（完成）
* 多线程爬虫
* 切换user-agent
* 从ip池中获取ip，免费代理
* 数据分析（完成）
* 绘制（完成）

# 数据分析

## 热门博主

这里是在2017-05-20晚上爬取按照积分排名前3000位的博主，本文是在3000条博主的用户信息，23万条博主的活动信息的基础上进行数据分析的，此数据基于学习的目的，不用于商业目的；

本来打算对具体动态信息做个排名，不过考虑到可能会涉及隐私，所以放弃，全文分析均为宏观分析；

### 各个博主的出生地

3000位博主中，只有546位填了家乡，比例为18%；

![](http://odwv9d2u8.bkt.clouddn.com/17-5-23/4748998-file_1495496762625_e3a2.png)

可以看的出来，湖北和河南的人数最多，广东四川北京山东属于第二梯队；

### 各个博主的现居住地

3000位博主中，只有802位填了现在的居住地，比例为27%；

![](http://odwv9d2u8.bkt.clouddn.com/17-5-23/94656922-file_1495496969213_10f3e.png)

可以发现，这个时候，博主的工作地点变成了IT比较发达的省份，像是北京，广东，浙江，上海，四川；

### 各个园龄所占的人数

![](http://odwv9d2u8.bkt.clouddn.com/17-5-23/94921794-file_1495497081535_d25.png)

大部分在博客园的时间在4-12年的时间，其中5年最多

### 工作职位的统计

206人填了信息，比例为7%

![](http://odwv9d2u8.bkt.clouddn.com/17-5-23/49252264-file_1495497299758_151f.png)

这里取的是排名前20位的职位，其中因为描述的不同导致结果存在些许不同，但可以发现其中软件工程师居多

### 工作单位的统计

116人填了信息，5个腾讯，2个Autodesk，2个武汉大学，其余均不重复，其中100人在工作，16人在大学

### 上一次发布博客的时间

1258个日子，最近一天为2017-05-20，最远一天为2005-04-14

![](http://odwv9d2u8.bkt.clouddn.com/17-5-23/97898692-file_1495497467699_111dd.png)

其中越靠后表明上一次发布博客的时间离现在越近，不过不难发现依然存在挺多的人上次发布博客的时间离现在比较远。

### 多少人进行了迁移（出生地跑到现居住地）

395人，此方法是按照出生地不等于现在居住地计算得出

### 结婚

107人填了信息，占整体比例为4%

![](http://odwv9d2u8.bkt.clouddn.com/17-5-23/6128447-file_1495497789039_1222.png)

单身比例最高，占了将近2/3，已婚次之

### 动态信息分布

![](http://odwv9d2u8.bkt.clouddn.com/17-5-23/41873138-file_1495497939215_4113.png)

发表话题和博客占用比例大体相当

### 分数平均值

以300为单位

![](http://odwv9d2u8.bkt.clouddn.com/17-5-23/42956737-file_1495498096178_14281.png)

可以发现，前300名大幅拉开和后面的差距

### 博客数量和分数、粉丝的关系

![](http://odwv9d2u8.bkt.clouddn.com/17-5-23/14634859-file_1495498198466_d2a3.png)

博客数量和分数并不是一个线性关系，表明并不是发表的博客数量越高分数就越高，不过貌似粉丝数量和分数存在些许关系。

### 总结

这个项目是我在工作之余花了一周的时间一变学习一边写出来的，其利用Scrapy爬虫框架来实现，过程中也走了些弯路，基本都是靠不停的查找资料来解决问题。项目并不困难，数据分析也比较简单，在后面可以加上词频分析等等，不过因为最近要开始找工作了，所以暂时要放置一段时间了。

文中如果有错误，请及时指出。
	
	

